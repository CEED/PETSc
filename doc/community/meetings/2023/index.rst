:orphan:

.. _2023_meeting:

*************************
2023 Annual PETSc Meeting
*************************

June 5-7, 2023 at the `Hermann Hall Conference Center <https://www.iit.edu/event-services/meeting-spaces/hermann-hall-conference-center>`__
(3241 South Federal Street, Chicago IL)
on the campus of `Illinois Institute of Technology (IIT) <https://www.iit.edu>`__ in Chicago.
Easy access from the hotels via the Chicago Elevated `Green <https://www.transitchicago.com/greenline>`__ or `Red <https://www.transitchicago.com/redline>`__ Lines.
`Parking use B5, 2nd & Federal St. <https://www.iit.edu/cbsc/parking/visitor-and-event-parking>`__.

Please test for Covid before attending the meeting and
mask while traveling to the meeting.

In addition to a newbie user tutorial and a :any:`newbie_developer_workshop`, the meeting will include a "speed dating" session where users can ask questions of developers (and each other) about technical details of their particular simulations. Finally the meeting will be interspersed with short mini tutorials that will dive down into particular aspects of PETSc that users may not be familiar with.

Meeting times
-------------
* Monday June 5: 1 pm to 5 pm
* Tuesday June 6: 10:15 am to 5 pm
* Wednesday June 7: 9 am to 3 pm

PETSc newbie user lightning tutorial:

* Monday June 5: 10 am to 12 pm

PETSc :any:`newbie_developer_workshop`

* Tuesday June 6: 9 am to 10 am


Registration
------------
Please register at `EventBrite <https://www.eventbrite.com/e/petsc-2023-user-meeting-tickets-494165441137>`__ to save your seat. 100 dollar registration fee for breaks and lunches; this can be skipped if you cannot afford it.

Submit a presentation
---------------------
`Submit an abstract  <https://docs.google.com/forms/d/e/1FAIpQLSesh47RGVb9YD9F1qu4obXSe1X6fn7vVmjewllePBDxBItfOw/viewform>`__ by May 1st (but preferably now) to be included in the schedule.  We welcome talks from all perspectives, including those who

* contribute to PETSc,
* use PETSc in their applications or libraries,
* develop the libraries and packages `called from PETSc <https://petsc.org/release/install/external_software/>`, and even
* those who are curious about using PETSc in their applications.


Suggested hotels
----------------

* `Receive IIT hotel discounts. <https://www.iit.edu/procurement-services/purchasing/preferred-and-contract-vendors/hotels>`__

* More Expensive

  * `Hilton Chicago <https://www.hilton.com/en/hotels/chichhh-hilton-chicago/?SEO_id=GMB-AMER-HI-CHICHHH&y_source=1_NzIxNzU2LTcxNS1sb2NhdGlvbi53ZWJzaXRl>`__ 720 S Michigan Ave, Chicago

  * `Hotel Blake, an Ascend Hotel Collection Member <https://www.choicehotels.com/illinois/chicago/ascend-hotels/il480>`__   500 S Dearborn St, Chicago, IL 60605

  * `The Blackstone, Autograph Collection <https://www.marriott.com/en-us/hotels/chiab-the-blackstone-autograph-collection/overview/?scid=f2ae0541-1279-4f24-b197-a979c79310b0>`__   636 South Michigan Avenue Lobby Entrance On, E Balbo Dr, Chicago

* Inexpensive

  * `Travelodge by Wyndham Downtown Chicago <https://www.wyndhamhotels.com/travelodge/chicago-illinois/travelodge-hotel-downtown-chicago/overview?CID=LC:TL::GGL:RIO:National:10073&iata=00093796>`__ 65 E Harrison St, Chicago

  * `The Congress Plaza Hotel & Convention Center <https://www.congressplazahotel.com/?utm_source=local-directories&utm_medium=organic&utm_campaign=travelclick-localconnect>`__ 520 S Michigan Ave, Chicago

  * `Hilton Garden Inn Chicago Downtown South Loop <https://www.hilton.com/en/hotels/chidlgi-hilton-garden-inn-chicago-downtown-south-loop/?SEO_id=GMB-AMER-GI-CHIDLGI&y_source=1_MTI2NDg5NzktNzE1LWxvY2F0aW9uLndlYnNpdGU%3D>`__ 55 E 11th St, Chicago

Tentative Agenda
----------------

Monday, June 5
^^^^^^^^^^^^^^

+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| Time       | Title                                                                                                                     | Speaker                  |
+============+===========================================================================================================================+==========================+
| 10:00 am   | Newbie tutorial (optional)                                                                                                |                          |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 11:30 am   | Follow-up questions and meetings                                                                                          |                          |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 12:00 am   | **Lunch** for tutorial attendees and early arrivees                                                                       |                          |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 1:00 pm    | *Some thoughts on the future of PETSc*                                                                                    | `Barry Smith`_           | 
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 1:30 pm    | *A new nonhydrostatic capability for MPAS-Ocean*                                                                          | `Sara Calandrini`_       |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 2:00 pm    | Presentation                                                                                                              |                          |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 2:30 pm    | Mini tutorial: *petsc4py*                                                                                                 |                          |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 2:45 pm    | **Coffee Break**                                                                                                          |                          |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 3:00 pm    | *Towards enabling digital twins capabilities for a cloud chamber*                                                         | `Vanessa Lopez-Marrero`_ |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 3:30 pm    | Presentation                                                                                                              |                          |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 4:00 pm    | *Software Development and Deployment Including PETSc*                                                                     | `Tim Steinhoff`_         |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 4:30 pm    | Mini tutorial: *PNODE, PyTorch, and petsc4py*                                                                             | Hong Zhang (Mr.)         |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 4:45 pm    | End of first day                                                                                                          |                          |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+

Tuesday, June 6
^^^^^^^^^^^^^^^

+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| Time       | Title                                                                                                                     | Speaker                  |
+============+===========================================================================================================================+==========================+
|            |                                                                                                                           |                          |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 9:00 am    | Newbie Developer Workshop (optional)                                                                                      |                          |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 10:00 am   | **Coffee Break**                                                                                                          |                          |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 10:15 am   | *Experiences in solving nonlinear eigenvalue problems with SLEPc*                                                         | `Jose E. Roman`_         |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 10:45 am   | *MPI Multiply Threads*                                                                                                    | `Hui Zhou`_              |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 11:15 am   | Mini tutorial: *PETSc on the GPU*                                                                                         | Junchao Zhang            |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 11:30 am   | *AMD GPU benchmarking, documentation, and roadmap*                                                                        | `Justin Chang`_          |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 12:00 pm   | **Lunch**                                                                                                                 |                          |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 1:00 pm    | *Transparent Asynchronous Compute Made Easy With PETSc*                                                                   | `Jacob Faibussowitsch`_  |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 1:30 pm    | *Scalable cloud-native thermo-mechanical solvers using PETSc*                                                             | `Ashish Patel`_          |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 2:00 pm    | Presentation                                                                                                              |                          |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 2:30 pm    | Mini tutorial: *DMPlex*                                                                                                   | Matt Knepley             |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 2:45 pm    | **Coffee Break**                                                                                                          |                          |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 3:00 pm    | *A mimetic finite difference based quasi-static magnetohydrodynamic solver for force-free plasmas in tokamak disruptions* | `Zakariae Jorti`_        |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 3:30 pm    | *High-order FEM implementation in AMReX using PETSc*                                                                      | `Alex Grant`_            |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 3:50 pm    | *Scalable Riemann Solvers with the Discontinuous Galerkin Method for Hyperbolic Network Simulation*                       | `Aidan Hamilton`_        |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 4:15 pm    | Mini tutorial: *DMNetwork*                                                                                                | `Hong Zhang (Ms.)`_      |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 4:30 pm    | End of second day                                                                                                         |                          |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+

Wednesday, June 7
^^^^^^^^^^^^^^^^^

+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| Time       | Title                                                                                                                     | Speaker                  |
+============+===========================================================================================================================+==========================+
| 9:00 am    | *XGCm: An Unstructured Mesh Gyrokinetic Particle-in-cell Code for Exascale Fusion Plasma Simulations*                     | `Chonglin Zhang`_        |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 9:30 am    | *PETSc-PIC: A Structure-Preserving Particle-In-Cell Method for Electrostatic Solves*                                      | `Daniel Finn`_           |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 10:00 am   | Mini tutorial: *DMSwarm*                                                                                                  | Joseph Pusztay           |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 10:15 am   | **Coffee Break**                                                                                                          |                          |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 10:30 am   | *PETSc in the Ionosphere*                                                                                                 | `Matt Young`_            |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 11:00 am   | Presentation                                                                                                              |                          |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 11:30 am   | Mini tutorial: *TaoADMM*                                                                                                  | `Hansol Suh`_            |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 11:45 am   | **Lunch**                                                                                                                 |                          |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 12:45 pm   | Presentation                                                                                                              |                          |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 1:15 pm    | Presentation                                                                                                              |                          |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 1:45 pm    | *Performance Portable l-BFGS in PETSc/TAO*                                                                                | `Toby Isaac`_            |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 2:15 pm    | *From the trenches: porting mef90*                                                                                        | `Blaise Bourdin`_        |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 2:45 pm    | Wrap up                                                                                                                   |                          | 
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+
| 3:00 pm    | End of meeting                                                                                                            |                          |
+------------+---------------------------------------------------------------------------------------------------------------------------+--------------------------+

.. _newbie_developer_workshop:

Newbie Developer Workshop
-------------------------

Tuesday June 6, at 9 am. Some of the topics to be covered.

* Exploring the developer documentation https://petsc.org/main/developers

* Preparing a contribution, PETSc style and git commit organization,

  * for source code

  * for documentation

* Submitting and monitoring a merge request (MR)

* Reviewing some one elses MR

* Adding new Fortran and Python function bindings

* PETSc's

  * configure system

  * compiler system, and

  * testing system including the GitLab CI

* Any other topics requested by potential contributors

Abstracts
---------

.. _`Blaise Bourdin`:

.. topic:: *Blaise Bourdin*, **From the trenches: porting mef90**

    mef90 is a distributed three-dimensional unstructured finite-element
    implementation of various phase-field models of fracture. In this talk,
    I will share experience gained while porting mef90 from petsc 3.3 to 3.18.

.. _`Sara Calandrini`:

.. topic:: *Sara Calandrini*, Darren Engwirda, Luke Van Roekel, **A new nonhydrostatic capability for MPAS-Ocean**

    The Model for Prediction Across Scales-Ocean (MPAS-Ocean) is an
    open-source, global ocean model and is one component within the Department
    of Energy’s E3SM framework, which includes atmosphere, sea-ice, and
    land-ice models. In this work, a new formulation for the ocean model is
    presented that solves the nonhydrostatic, incompressible Boussinesq
    equations on unstructured meshes. The introduction of this nonhydrostatic
    capability is necessary for the representation of fine-scale dynamical
    processes, including resolution of internal wave dynamics and large eddy
    simulations. Compared to the standard, hydrostatic formulation,
    a nonhydrostatic pressure solver and a vertical momentum equation are
    added, where the PETSc (Portable Extensible Toolkit for Scientific
    Computation) library is used for the inversion of a large sparse system for
    the nonhydrostatic pressure. Numerical results comparing the solutions of
    the hydrostatic and nonhydrostatic models are presented, and the parallel
    efficiency and accuracy of the time-stepper are evaluated.

.. _`Justin Chang`:

.. topic:: *Justin Chang*, **AMD GPU benchmarking, documentation, and roadmap**

    This talk comprises of three parts. First, we present an overview of some
    relatively new training documentation like the "AMD lab notes" to enable
    current and potential users of AMD GPUs into getting the best experience
    out of their applications or algorithms. Second, we briefly discuss
    implementation details regarding the PETSc HIP backend introduced into the
    PETSc library late last year and present some performance benchmarking data
    on some of the AMD hardware. Lastly, we give a preview of the upcoming
    MI300 series APU and how software developers can prepare to leverage this
    new type of accelerator.

.. _`Jacob Faibussowitsch`:

.. topic:: *Jacob Faibussowitch*, **Transparent Asynchronous Compute Made Easy With PETSc**

    Asynchronous GPU computing has historically been difficult to scalably
    integrate at the library level. We provide an update on recent work
    implementing a fully asynchronous framework in PETSc. We give detailed
    performance comparisons and provide a demo to showcase the effectiveness
    and ease-of-use of the proposed model.

.. _`Daniel Finn`:

.. topic:: *Daniel Finn*, **PETSc-PIC: A Structure-Preserving Particle-In-Cell Method for Electrostatic Solves**

    Numerical solutions to the Vlasov-Poisson equations have important
    applications in the fields of plasma physics, solar physics and cosmology.
    The goal of this research is to develop a structure-preserving,
    electrostatic and gravitational Vlasov-Poisson(-Landau) model using the
    Portable, Extensible Toolkit for Scientific Computation (PETSc) and study
    the presence of Landau damping in a variety of systems, such as
    thermonuclear fusion reactors and galactic dynamics. The PETSc
    Particle-In-Cell (PETSc-PIC) model is a highly-scalable,
    structure-preserving PIC method with multigrid capabilities. In the PIC
    method, a hybrid discretization is constructed with a grid of finitely
    supported basis functions to represent the electric, magnetic and/or
    gravitational fields, and a distribution of delta functions to represent
    the particle field. Collisions are added to the formulation by means of
    a particle-basis Landau collision operator, recently added to the PETSc
    library.

.. _`Alex Grant`:

.. topic:: *Alex Grant*, Karthik Chockalingam, Xiaohu Guo, **High-order FEM implementation in AMReX using PETSc**

    AMReX is a C++ block-structured framework for adaptive mesh refinement,
    typically used for finite difference or finite volume codes.  We describe
    a first attempt at a finite element implementation in AMReX using PETSc.
    AMReX splits the domain of uniform elements into rectangular boxes at each
    refinement level, with higher levels overlapping rather than replacing
    lower levels, and with each level solved independently.  AMReX boxes can be
    cell-centred or nodal, we use cell centred boxes to represent the geometry
    and mesh and nodal boxes to identify nodes to constrain and store results
    for visualisation.  We convert AMReX’s independent spatial indicies into
    a single global index then use MATMPIAIJ to assemble the system matrix per
    refinement level.  In an unstructured grid, isoparametric mapping is
    required for each element, the use of a structured grid avoids both this
    and indirect addressing, which provides significant potential performance
    advantages.  We have solved time-dependent parabolic equations and seen
    performance gains compared to unstructured finite elements.  Further
    developments are planned to include arbitrary higher order schemes and
    multi-level hp refinement with arbitrary hanging nodes.  PETSc uses AMReX
    domain decomposition to partition the matrix and right-hand vectors.  For
    each higher level, not all of the domain will be refined, but AMReX’s
    indicies cover the whole space - this poses an indexing challenge and can
    lead to over-allocation of memory.  It is still to be explored whether DM
    data structures would provide a benefit over MATMPIAIJ.

.. _`Aidan Hamilton`:

.. topic:: *Aidan Hamilton*, Jing-Mei Qiu, Hong Zhang, **Scalable Riemann Solvers with the Discontinuous Galerkin Method for Hyperbolic Network Simulation**

    We develop a set of highly efficient and effective computational algorithms
    and simulation tools for fluid simulations on a network. The mathematical
    models are a set of hyperbolic conservation laws on edges of a network, as
    well as coupling conditions on junctions of a network. For example, the
    shallow water system, together with flux balance and continuity conditions
    at river intersections, model water flows on a river network. The
    computationally ac- curate and robust discontinuous Galerkin methods,
    coupled with explicit strong stability preserving Runge-Kutta methods, are
    imple- mented for simulations on network edges. Meanwhile, linear and
    nonlinear scalable Riemann solvers are being developed and imple- mented at
    network vertices. These network simulations result in tools built using
    PETSc and DMNetwork software libraries for the scientific community in
    general. Simulation results of a shallow water system on a Mississippi
    river network with over one billion network variables are performed on an
    extreme- scale computer using up to 8,192 processor with an optimal
    parallel efficiency. Further potential applications include traffic flow
    sim- ulations on a highway network and blood flow simulations on a arterial
    network, among many others

.. _`Toby Isaac`:

.. topic:: *Toby Isaac*, Alp Dener, Hansol Suh, Todd Munson, **Performance Portable l-BFGS in PETSc/TAO**

    We describe recent work reformulating limited memory BFGS methods in
    compact dense represeentations that allow for kernel fusion in GPU
    implementations, and present performance measurements comparing several
    strategies for handling the two important operations: applying the l-BFGS
    Jacobian (inverse) approximation and updating the data structures.

.. _`Zakariae Jorti`:

.. topic:: *Zakariae Jorti*, Qi Tang, Konstantin Lipnikov, Xianzhu Tang, **A mimetic finite difference based quasi-static magnetohydrodynamic solver for force-free plasmas in tokamak disruptions**

    Force-free plasmas are a good approximation in the low-beta case where the
    plasma pressure is tiny compared with the magnetic pressure. On time scales
    long compared with the transit time of Alfvén waves, the evolution of
    a force-free plasma is most efficiently described by a quasi-static
    magnetohydrodynamic (MHD) model, which ignores the plasma inertia. In this
    work, we consider a regularized quasi-static MHD model for force-free
    plasmas in tokamak disruptions and propose a mimetic finite difference
    (MFD) algorithm, which is targeted at applications such as the cold
    vertical displacement event (VDE) of a major disruption in an ITER-like
    tokamak reactor. In the case of whole device modeling, we further consider
    the two sub-domains of the plasma region and wall region and their coupling
    through an interface condition. We develop a parallel, fully implicit, and
    scalable MFD solver based on PETSc and its DMStag data structure for the
    discretization of the five-field quasi-static perpendicular plasma dynamics
    model on a 3D structured mesh. The MFD spatial discretization is coupled
    with a fully implicit DIRK scheme. The full algorithm exactly preserves the
    divergence-free condition of the magnetic field under a generalized Ohm’s
    law. The preconditioner employed is a four-level fieldsplit preconditioner,
    which is created by combining separate preconditioners for individual
    fields, that calls multigrid or direct solvers for sub-blocks or exact
    factorization on the separate fields. The numerical results confirm the
    divergence-free constraint is strongly satisfied and demonstrate the
    performance of the fieldsplit preconditioner and overall algorithm. The
    simulation of ITER VDE cases over the actual plasma current diffusion time
    is also presented.

.. _`Vanessa Lopez-Marrero`:

.. topic:: *Vanessa Lopez-Marrero*, Kwangmin Yu, Tao Zhang, Mohammad Atif, Abdullah Al Muti Sharfuddin, Fan Yang, Yangang Liu, Meifeng Lin, Foluso Ladeinde, Lingda Li, **Towards enabling digital twins capabilities for a cloud chamber**

    Particle-resolved direct numerical simulations (PR-DNS), which resolve not
    only the smallest turbulent eddies but also track the development and
    motion of individual particles, are an essential tool for studying
    aerosol-cloud-turbulence interactions.  For instance, PR-DNS may complement
    experimental facilities designed to study key physical processes in
    a controlled environment and therefore serve as digital twins for such
    cloud chambers.  In this talk we will present our ongoing work aimed at
    enabling the use of PR-DNS for this purpose.  We will describe the physical
    model being used, which consists of a set of fluid dynamics equations for
    air velocity, temperature, and humidity, coupled with a set of equations
    for particle (i.e., droplet) growth/tracing.  The numerical method used to
    solve the model, which employs PETSc solvers in its implementation, will be
    discussed, as well as our current efforts to assess performance and
    scalability of the numerical solver.


.. _`Ashish Patel`:

.. topic:: *Ashish Patel*, Jeremy Theler, Francesc Levrero-Florencio, Nabil Abboud, Mohammad Sarraf Joshaghani, Scott McClennan, **Scalable cloud-native thermo-mechanical solvers using PETSc**

    In this talk, we present how the Ansys OnScale team is using PETSc to
    develop finite element-based thermo-mechanical solvers for scalable
    nonlinear simulations on the cloud. We will first provide an overview of
    features available in the solver and then discuss how some of the PETSc
    objects, like DMPlex and TS, have helped us speed up our development
    process. We will also talk about the workarounds we have incorporated to
    address the current limitations of some of the functions from DMPlex for
    our use cases involving multi-point constraints and curved elements.
    Finally, we demonstrate how PETSc’s linear solvers scale on multi-node
    cloud instances.

.. _`Jose E. Roman`:

.. topic:: *Jose E. Roman*, **Experiences in solving nonlinear eigenvalue problems with SLEPc**

    One of the unique features of SLEPc is the module for the general nonlinear
    eigenvalue problem (NEP), where we want to compute a few eigenvalues and
    corresponding eigenvectors of a large-scale parameter-dependent matrix
    T(lambda). In this talk, we will illustrate the use of NEP in the context
    of two applications, one of them coming from the characterization of
    resonances in nanophotonic devices, and the other one from a problem in
    aeroacoustics.

.. _`Barry Smith`:

.. topic:: *Barry Smith*, **Some thoughts on the future of PETSc**:

    How will PETSc evolve and grow in the future? How can PETSc algorithms and
    simulations be integrated into the emerging world of machine learning and
    deep neural networks? I will provide an informal discussion of these topics
    and my personal thoughts.

.. _`Tim Steinhoff`:

.. topic:: *Tim Steinhoff*, Volker Jacht, **Software Development and Deployment Including PETSc**

    Once it is decided that PETSc shall handle certain numerical subtasks in
    your own software the question may arise how to smoothly incorporate PETSc
    into the overall software development and deployment processes. In this
    talk we present our approach how to handle such a situation for the code
    family AC2 which is developed and distributed by GRS. AC2 is used to
    simulate the behavior of nuclear reactors during operation, transients,
    design basis and beyond design basis accidents up to radioactive releases
    to the environment. The talk addresses our experiences, what challenges had
    to be overcome, and how we make use of GitLab, CMake, and Docker techniques
    to establish a clean incorporation of PETSc into our software development
    cycle.

.. _`Hansol Suh`:

.. topic:: *Hansol Suh*, **TaoADMM**

    In this tutorial, we will be giving an introduction to ADMM algorithm on
    TAO. It will include walking through ADMM algorithm with some real-life
    example, and tips on setting up framework to solve ADMM on PETSc/TAO.

.. _`Matt Young`:

.. topic:: *Matt Young*, **PETSc in the Ionosphere**

    A planet's ionosphere is the region of its atmosphere in which a fraction
    of the constituent atoms or molecules have separated into positive ions and
    electrons. Earth's ionosphere extends from roughly 85 km during the day
    (higher at night) to the edge of space. This partially ionized regime
    exhibits collective behavior and supports electromagnetic phenomena that do
    not exist in the neutral (i.e., unionized) atmosphere. Furthermore, the
    abundance of neutral atoms and molecules leads to phenomena that do not
    exist in the fully ionized space environment. In a relatively narrow
    altitude range of Earth's ionosphere called the "E region", electrons
    behave as typical charged particles -- moving in response to combined
    electric and magnetic fields -- while ions collide too frequently with
    neutral molecules to respond to the magnetic field. This difference leads
    to the Farley-Buneman instability when the local electric field is strong
    enough. The Farley-Buneman instability regularly produces irregularities in
    the charged-particle densities that are strong enough to reflect radio
    signals, and recent research suggests that the fully developed turbulent
    structures are capable of disrupting GPS communication.

    The Electrostatic Parallel Particle-in-Cell (EPPIC) numerical simulation
    self-consistently models instability growth and evolution in the E-region
    ionosphere. The simulation includes a hybrid mode that treats electrons as
    a fluid and treats ions as particles. The particular fluid electron model
    requires the solution of an elliptic partial differential equation for the
    electrostatic potential at each time step, which we represent as a linear
    system that the simulation solves with PETSc. This presentation will
    describe original development of the 2D hybrid simulation, previous
    results, recently efforts to extend to 3D, and implications to modeling GPS
    scintillation.

    The Electrostatic Parallel Particle-in-Cell (EPPIC) numerical simulation
    self-consistently models instability growth and evolution in the E-region
    ionosphere. The simulation includes a hybrid mode that treats electrons as
    a fluid and treats ions as particles. The particular fluid electron model
    requires the solution of an elliptic partial differential equation for the
    electrostatic potential at each time step, which we represent as a linear
    system that the simulation solves with PETSc. This presentation will describe
    original development of the 2D hybrid simulation, previous results, recently
    efforts to extend to 3D, and implications to modeling GPS scintillation.

.. _`Chonglin Zhang`:

.. topic:: *Chonglin Zhang*, Cameron W. Smith, Mark S. Shephard, **XGCm: An Unstructured Mesh Gyrokinetic Particle-in-cell Code for Exascale Fusion Plasma Simulations**

    We report the development of XGCm, a new distributed unstructured mesh
    gyrokinetic particle-in-cell (PIC) code, short for x-point included
    gyrokinetic code mesh-based. The code adopts the physical algorithms of the
    well-established XGC code. It is intended as a testbed for experimenting
    with new numerical and computational algorithms, which can eventually be
    adopted in XGC and other PIC codes. XGCm is developed on top of several
    open-source libraries, including Kokkos, PETSc, Omega, and PUMIPic. Omega
    and PUMIPic rely on Kokkos to interact with the GPU accelerator, while
    PETSc solves the gyrokinetic Poisson equation on either CPU or GPU. We
    first discuss the numerical algorithms of our mesh-centric approach for
    performing PIC calculations. We then present code validation study using
    the cyclone base case with ion temperature gradient turbulence (case 5 from
    Burckel, etc. Journal of Physics: Conference Series 260, 2010, 012006).
    Finally, we discuss the performance of XGCm and present weak scaling
    results using up to full system (27,648 GPUs) of the Oak Ridge National
    Laboratory’s Summit supercomputer. Overall, XGCm executes all PIC
    operations on the GPU accelerators and exhibits good performance and
    portability.

.. _`Hong Zhang (Ms.)`:

.. topic:: *Hong Zhang*, **PETSc DMNetwork: A Library for Scalable Network PDE-Based Multiphysics Simulation**

    We present DMNetwork, a high-level set of routines included in the PETSc
    library for the simulation of multiphysics phenomena over large-scale
    networked systems. The library aims at applications that have networked
    structures such as the ones found in electrical, water and traffic
    distribution systems. DMNetwork provides data and topology management,
    parallelization for multiphysics systems over a network, and hierarchical
    and composable solvers to exploit the problem structure.  DMNetwork eases
    the simulation development cycle by providing the necessary infrastructure
    through simple abstractions to define and query the network components.

.. _`Hui Zhou`:

.. topic:: *Hui Zhou*, **MPI Multiply Threads**

    In the traditional MPI+Thread programming paradigm, MPI and OpenMP each
    form their own disjoint parallelization. MPI is unaware of the thread
    context. The requirement of thread safety and message ordering forces MPI
    library to blindly add critical sections, unnecessarily serializing the
    code. On the other hand, OpenMP cannot use MPI for inter-thread
    communications. Developers often need hand-roll their own algorithms for
    collective operations and non-blocking synchronizations.

    MPICH recently added a few extensions to address the root issues in
    MPI+Thread. The first extension, MPIX stream, allows applications to
    explicitly pass the thread context into MPI. The second extension, thread
    communicator, allows individual threads in an OpenMP parallel region to use
    MPI for inter-thread communications. In particular, this allows an OpenMP
    program to use PETSc within an parallel region.

    Instead of MPI+Thread, we refer to this new pattern as MPI x Thread.
