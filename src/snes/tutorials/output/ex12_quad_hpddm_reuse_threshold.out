  0 SNES Function norm 10.2781 
    0 KSP Residual norm 10.1621 
    1 KSP Residual norm 2.54801 
    2 KSP Residual norm 1.58203 
    3 KSP Residual norm 0.838201 
  Linear solve converged due to CONVERGED_RTOL iterations 3
  1 SNES Function norm 1.05621 
    0 KSP Residual norm 0.838201 
    1 KSP Residual norm 0.650324 
    2 KSP Residual norm 0.360822 
    3 KSP Residual norm 0.20776 
    4 KSP Residual norm 0.0960564 
    5 KSP Residual norm 0.0567234 
  Linear solve converged due to CONVERGED_RTOL iterations 5
  2 SNES Function norm 0.0576719 
    0 KSP Residual norm 0.0567234 
    1 KSP Residual norm 0.0496642 
    2 KSP Residual norm 0.0369519 
    3 KSP Residual norm 0.0131327 
    4 KSP Residual norm 0.0099831 
    5 KSP Residual norm 0.00717354 
    6 KSP Residual norm 0.0023424 
  Linear solve converged due to CONVERGED_RTOL iterations 6
  3 SNES Function norm 0.00345787 
    0 KSP Residual norm 0.0023424 
    1 KSP Residual norm 0.0017353 
    2 KSP Residual norm 0.00105718 
    3 KSP Residual norm 0.000557015 
    4 KSP Residual norm 0.000351944 
    5 KSP Residual norm 0.000179459 
  Linear solve converged due to CONVERGED_RTOL iterations 5
  4 SNES Function norm 0.000200279 
    0 KSP Residual norm 0.000179459 
    1 KSP Residual norm 0.000131462 
    2 KSP Residual norm 6.26777e-05 
    3 KSP Residual norm 3.85537e-05 
    4 KSP Residual norm 3.0915e-05 
    5 KSP Residual norm 1.51535e-05 
  Linear solve converged due to CONVERGED_RTOL iterations 5
  5 SNES Function norm 2.6724e-05 
    0 KSP Residual norm 1.51535e-05 
    1 KSP Residual norm 6.6192e-06 
    2 KSP Residual norm 4.47144e-06 
    3 KSP Residual norm 3.99377e-06 
    4 KSP Residual norm 1.5097e-06 
  Linear solve converged due to CONVERGED_RTOL iterations 4
  6 SNES Function norm 2.32971e-06 
    0 KSP Residual norm 1.5097e-06 
    1 KSP Residual norm 9.72569e-07 
    2 KSP Residual norm 7.30195e-07 
    3 KSP Residual norm 3.51447e-07 
    4 KSP Residual norm 1.92251e-07 
    5 KSP Residual norm 9.94109e-08 
  Linear solve converged due to CONVERGED_RTOL iterations 5
  7 SNES Function norm 1.05177e-07 
    0 KSP Residual norm 9.94109e-08 
    1 KSP Residual norm 5.71902e-08 
    2 KSP Residual norm 2.33661e-08 
    3 KSP Residual norm 1.62885e-08 
    4 KSP Residual norm 1.13486e-08 
    5 KSP Residual norm 6.76605e-09 
  Linear solve converged due to CONVERGED_RTOL iterations 5
  8 SNES Function norm 1.34519e-08 
L_2 Error: 0.000629251
Nonlinear solve converged due to CONVERGED_FNORM_RELATIVE iterations 8
SNES Object: 4 MPI processes
  type: newtonls
  maximum iterations=50, maximum function evaluations=10000
  tolerances: relative=1e-08, absolute=1e-50, solution=1e-08
  total number of linear solver iterations=38
  total number of function evaluations=9
  norm schedule ALWAYS
  SNESLineSearch Object: 4 MPI processes
    type: bt
      interpolation: cubic
      alpha=1.000000e-04
    maxstep=1.000000e+08, minlambda=1.000000e-12
    tolerances: relative=1.000000e-08, absolute=1.000000e-15, lambda=1.000000e-08
    maximum iterations=40
  KSP Object: 4 MPI processes
    type: gmres
      restart=100, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
      happy breakdown tolerance 1e-30
    maximum iterations=10000, initial guess is zero
    tolerances:  relative=0.1, absolute=1e-50, divergence=10000.
    left preconditioning
    using PRECONDITIONED norm type for convergence test
  PC Object: 4 MPI processes
    type: hpddm
    levels: 2
    Neumann matrix attached? TRUE
    shared subdomain KSP between SLEPc and PETSc? FALSE
    coarse correction: DEFLATED
    on process #0, value (+ threshold if available) for selecting deflation vectors: 0 (0.1)
    grid and operator complexities: 1.0039 1.00046
    KSP Object: (pc_hpddm_levels_1_) 4 MPI processes
      type: preonly
      maximum iterations=10000, initial guess is zero
      tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
      left preconditioning
      using NONE norm type for convergence test
    PC Object: (pc_hpddm_levels_1_) 4 MPI processes
      type: shell
        no name
      linear system matrix = precond matrix:
      Mat Object: 4 MPI processes
        type: mpiaij
        rows=513, cols=513
        total: nonzeros=4345, allocated nonzeros=4345
        total number of mallocs used during MatSetValues calls=0
          not using I-node (on process 0) routines
    PC Object: (pc_hpddm_levels_1_) 4 MPI processes
      type: bjacobi
        number of blocks = 4
        Local solver information for first block is in the following KSP and PC objects on rank 0:
        Use -pc_hpddm_levels_1_ksp_view ::ascii_info_detail to display information for all blocks
      KSP Object: (pc_hpddm_levels_1_sub_) 1 MPI process
        type: preonly
        maximum iterations=10000, initial guess is zero
        tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
        left preconditioning
        using NONE norm type for convergence test
      PC Object: (pc_hpddm_levels_1_sub_) 1 MPI process
        type: lu
          out-of-place factorization
          tolerance for zero pivot 2.22045e-14
          matrix ordering: nd
          factor fill ratio given 5., needed 1.9272
            Factored matrix follows:
              Mat Object: (pc_hpddm_levels_1_sub_) 1 MPI process
                type: seqaij
                rows=109, cols=109
                package used to perform factorization: petsc
                total: nonzeros=1509, allocated nonzeros=1509
                  not using I-node routines
        linear system matrix = precond matrix:
        Mat Object: (pc_hpddm_levels_1_sub_) 1 MPI process
          type: seqaij
          rows=109, cols=109
          total: nonzeros=783, allocated nonzeros=783
          total number of mallocs used during MatSetValues calls=0
            not using I-node routines
      linear system matrix = precond matrix:
      Mat Object: 4 MPI processes
        type: mpiaij
        rows=513, cols=513
        total: nonzeros=4345, allocated nonzeros=4345
        total number of mallocs used during MatSetValues calls=0
          not using I-node (on process 0) routines
      KSP Object: (pc_hpddm_coarse_) 2 MPI processes
        type: preonly
        maximum iterations=10000, initial guess is zero
        tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
        left preconditioning
        using NONE norm type for convergence test
      PC Object: (pc_hpddm_coarse_) 2 MPI processes
        type: redundant
          First (color=0) of 2 PCs follows
        linear system matrix = precond matrix:
        Mat Object: (pc_hpddm_coarse_) 2 MPI processes
          type: mpisbaij
          rows=2, cols=2
          total: nonzeros=2, allocated nonzeros=2
          total number of mallocs used during MatSetValues calls=0
              block size is 1
                KSP Object: (pc_hpddm_coarse_redundant_) 1 MPI process
                  type: preonly
                  maximum iterations=10000, initial guess is zero
                  tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
                  left preconditioning
                  using NONE norm type for convergence test
                PC Object: (pc_hpddm_coarse_redundant_) 1 MPI process
                  type: cholesky
                    out-of-place factorization
                    tolerance for zero pivot 2.22045e-14
                    matrix ordering: natural
                    factor fill ratio given 5., needed 1.
                      Factored matrix follows:
                        Mat Object: (pc_hpddm_coarse_redundant_) 1 MPI process
                          type: seqsbaij
                          rows=2, cols=2
                          package used to perform factorization: petsc
                          total: nonzeros=2, allocated nonzeros=2
                              block size is 1
                  linear system matrix = precond matrix:
                  Mat Object: 1 MPI process
                    type: seqsbaij
                    rows=2, cols=2
                    total: nonzeros=2, allocated nonzeros=2
                    total number of mallocs used during MatSetValues calls=0
                        block size is 1
    linear system matrix = precond matrix:
    Mat Object: 4 MPI processes
      type: mpiaij
      rows=513, cols=513
      total: nonzeros=4345, allocated nonzeros=4345
      total number of mallocs used during MatSetValues calls=0
        not using I-node (on process 0) routines
